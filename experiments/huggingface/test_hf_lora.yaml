experiment_name: hf-llama-lora
tags:
    - huggingface
model:
    model_id: meta-llama/Llama-3.2-1B
    dtype: float32
    rank: 4
    alpha: 8
data_loading:
    dataset_id: yahma/alpaca-cleaned
    max_length: 512
    instruction_columns: ["instruction", "input"]
    batch_size: 2
    train_sample: 10
    validation_sample: 20
training_config:
    epochs: 3
    loss: CrossEntropyLoss
    lr: 0.001
    run_on: mps
