# Dataset settings
dataset_id: "stanfordnlp/sst2"

# Model settings
model_name: "meta-llama/Llama-3.2-1B"
max_length: 128
dtype: "torch.bfloat16" # "torch.float32"

# Training hyperparameters
learning_rate: 2e-5
batch_size: 32
gradient_accumulation_steps: 1
gradient_checkpointing: False
num_epochs: 1
optim: "adamw_torch"

# LoRA setup
lora_r: 4
lora_alpha: 8
lora_dropout: 0.1
lora_bias: "none"
lora_target_modules: "all-linear"

# Other settings
seed: 23
output_dir: "thomas/experiments/pytorch/results/llama32-1b-bs32-bf16-ml128-r4-a8-adamw_torch"
report_to: "wandb"
wandb_project: "llama-finetuning"
save_strategy: "epoch"
logging_strategy: "steps"
logging_steps: 10
save_total_limit: 3
do_train: True
do_eval: False
