experiment_name: hf-llama-lora
tags:
    - huggingface
model:
    model_id: meta-llama/Llama-3.2-1B
    dtype: float32
    rank: 4
    alpha: 8
data_loading:
    dataset_id: timdettmers/openassistant-guanaco
    max_length: 512
    batch_size: 2
    train_sample: 100
    validation_sample: 20
training_config:
    output_dir: thomas/experiments/test_hf_lora
    epochs: 3
    loss: CrossEntropyLoss
    lr: 0.001
    run_on: mps
    save_strategy: steps
    save_steps: 25
    eval_strategy: "no"
