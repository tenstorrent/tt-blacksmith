experiment_name: pytorch-llama-lora
tags:
    - pytorch
model:
    model_id: meta-llama/Llama-3.2-1B
    dtype: bfloat16
    rank: 4
    alpha: 8
    lora_attn_modules: ["q_proj", "v_proj"]
data_loading:
    dataset_id: yahma/alpaca-cleaned
    cache_dir: data
    max_length: 512
    instruction_columns: ["instruction", "input"]
    batch_size: 2
    train_sample: 100
    validation_sample: 20
training_config:
    epochs: 3
    loss: CrossEntropyLoss
    optimizer: AdamW
    optimizer_kwargs:
      lr: 0.001
    run_on: mps
    do_validation: True
