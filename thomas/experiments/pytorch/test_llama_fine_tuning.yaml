    # Dataset settings
    dataset_id: "stanfordnlp/sst2"

    # Model settings
    model_name: "openlm-research/open_llama_3b"
    max_length: 128
    dtype: "torch.bfloat16"

    # Training hyperparameters
    learning_rate: 2e-5
    batch_size: 4
    gradient_accumulation_steps: 1
    gradient_checkpointing: False
    num_epochs: 1
    optim: "adamw_torch"

    # LoRA setup
    lora_r: 4
    lora_alpha: 8
    lora_dropout: 0.1
    lora_bias: "none"
    lora_target_modules: "all-linear"

    # Other settings
    seed: 23
    output_dir: "thomas/experiments/pytorch/results/open_llama-3b-bs4-bf16-ml128-r4-a8-adamw_torch"
    report_to: "wandb"
    wandb_project: "llama-finetuning"
    save_strategy: "epoch"
    logging_strategy: "steps"
    logging_steps: 10
    save_total_limit: 3
    do_train: True
    do_eval: True
